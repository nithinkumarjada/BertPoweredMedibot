{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b156dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7764c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.0/1.5 MB 1.3 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 0.1/1.5 MB 1.1 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 0.2/1.5 MB 1.2 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 0.2/1.5 MB 1.0 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 0.3/1.5 MB 1.3 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.6/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 0.7/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.1/1.5 MB 3.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 3.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in e:\\anacondafiles\\envs\\geekl\\lib\\site-packages (from nltk) (8.1.6)\n",
      "Collecting joblib (from nltk)\n",
      "  Obtaining dependency information for joblib from https://files.pythonhosted.org/packages/28/08/9dcdaa5aac4634e4c23af26d92121f7ce445c630efa0d3037881ae2407fb/joblib-1.3.1-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.3.1-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\anacondafiles\\envs\\geekl\\lib\\site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in e:\\anacondafiles\\envs\\geekl\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\manis\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/302.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 302.0/302.0 kB 9.4 MB/s eta 0:00:00\n",
      "Installing collected packages: joblib, nltk\n",
      "Successfully installed joblib-1.3.1 nltk-3.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c77fa27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontractions(phrase):\n",
    "    \"\"\"decontracted takes text and convert contractions into natural form.\n",
    "     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "\n",
    "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "\n",
    "    return phrase\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    # convert all the text into lower letters\n",
    "    # remove the words betweent brakets ()\n",
    "    # remove these characters: {'$', ')', '?', '\"', '’', '.',  '°', '!', ';', '/', \"'\", '€', '%', ':', ',', '('}\n",
    "    # replace these spl characters with space: '\\u200b', '\\xa0', '-', '/'\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = decontractions(text)\n",
    "    text = re.sub('[$)\\?\"’.°!;\\'€%:,(/]', '', text)\n",
    "    text = re.sub('\\u200b', ' ', text)\n",
    "    text = re.sub('\\xa0', ' ', text)\n",
    "    text = re.sub('-', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "512c9c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "#importing bert tokenizer and loading the trained question embedding extractor model\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "biobert_tokenizer = AutoTokenizer.from_pretrained(\"cambridgeltl/BioRedditBERT-uncased\")\n",
    "question_extractor_model1=tf.keras.models.load_model('question_extractor_model_2_11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c229fd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ./tf_gpt2_model_2_100.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer,TFGPT2LMHeadModel\n",
    "gpt2_tokenizer=GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tf_gpt2_model=TFGPT2LMHeadModel.from_pretrained(\"./tf_gpt2_model_2_100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558e8130",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing the faiss search\n",
    "qa=pd.read_pickle('./train_gpt_data.pkl')\n",
    "question_bert = qa[\"Q_FFNN_embeds\"].tolist()\n",
    "answer_bert = qa[\"A_FFNN_embeds\"].tolist()\n",
    "question_bert = np.array(question_bert)\n",
    "answer_bert = np.array(answer_bert)\n",
    "\n",
    "question_bert = question_bert.astype('float32')\n",
    "answer_bert = answer_bert.astype('float32')\n",
    "\n",
    "answer_index = faiss.IndexFlatIP(answer_bert.shape[-1])\n",
    "\n",
    "question_index = faiss.IndexFlatIP(question_bert.shape[-1])\n",
    "answer_index.add(answer_bert)\n",
    "question_index.add(question_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ca31a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function to prepare the data for gpt inference\n",
    "#https://github.com/ash3n/DocProduct\n",
    "def preparing_gpt_inference_data(question,question_embedding):\n",
    "  topk=20\n",
    "  scores,indices=answer_index.search(\n",
    "                  question_embedding.astype('float32'), topk)\n",
    "  q_sub=qa.iloc[indices.reshape(20)]\n",
    "  \n",
    "  line = '`QUESTION: %s `ANSWER: ' % (\n",
    "                        question)\n",
    "  encoded_len=len(gpt2_tokenizer.encode(line))\n",
    "  for i in q_sub.iterrows():\n",
    "    line='`QUESTION: %s `ANSWER: %s ' % (i[1]['question'],i[1]['answer']) + line\n",
    "    line=line.replace('\\n','')\n",
    "    encoded_len=len(gpt2_tokenizer.encode(line))\n",
    "    if encoded_len>=1024:\n",
    "      break\n",
    "  return gpt2_tokenizer.encode(line)[-1024:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b15274a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_answer(question,answer_len):\n",
    "  preprocessed_question=preprocess(question)\n",
    "  question_len=len(preprocessed_question.split(' '))\n",
    "  truncated_question=preprocessed_question\n",
    "  if question_len>500:\n",
    "    truncated_question=' '.join(preprocessed_question.split(' ')[:500])\n",
    "  encoded_question= biobert_tokenizer.encode(truncated_question)\n",
    "  max_length=256\n",
    "  padded_question=tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      [encoded_question], maxlen=max_length, padding='post')\n",
    "  question_mask=[[1 if token!=0 else 0 for token in question] for question in padded_question]\n",
    "  embeddings=question_extractor_model1({'question':np.array(padded_question),'question_mask':np.array(question_mask)})\n",
    "  gpt_input=preparing_gpt_inference_data(truncated_question,embeddings.numpy())\n",
    "  mask_start = len(gpt_input) - list(gpt_input[::-1]).index(4600) + 1\n",
    "  input=gpt_input[:mask_start+1]\n",
    "  if len(input)>(1024-answer_len):\n",
    "   input=input[-(1024-answer_len):]\n",
    "  gpt2_output=gpt2_tokenizer.decode(tf_gpt2_model.generate(input_ids=tf.constant([np.array(input)]),max_length=1024,temperature=0.7)[0])\n",
    "  answer=gpt2_output.rindex('`ANSWER: ')\n",
    "  return gpt2_output[answer+len('`ANSWER: '):]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36c1457d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the final function to generate answer assuming default answer length to be 20\n",
    "def final_func_1(question):\n",
    "  answer_len=50\n",
    "  return give_answer(question,answer_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c561f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the final function to calculate the required metric answer assuming default answer length to be 20\n",
    "def final_func_2(question,answer):\n",
    "  answer_len=20\n",
    "  generated_answer=give_answer(question,answer_len)\n",
    "  reference = [answer.split(' ')]\n",
    "  candidate = generated_answer.split(' ')\n",
    "  score = sentence_bleu(reference, candidate)\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ceb9dbf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer: yes sugar is absorbed through the skin and is absorbed through the skin through the skin and through the skin is not a problem for people with diabetes or other diseases that require a liver transplant or surgery to remove the liver it is a good idea to consult\n"
     ]
    }
   ],
   "source": [
    "#testing final_func1 on sample input\n",
    "answer=final_func_1(\"can sugar be absorbed through the skin \")\n",
    "print(\"answer:\",answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a93447",
   "metadata": {},
   "outputs": [],
   "source": [
    "'question_extractor_model_2_11'\n",
    "\"./tf_gpt2_model_2_100\"\n",
    "./train_gpt_data.pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
